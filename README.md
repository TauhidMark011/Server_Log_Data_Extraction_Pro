# ğŸ“Š Server Log Data Extraction and User History Database Update, By GUVI.

## ğŸ“– Project Overview 
The "Server Log Data Extraction and User History Database Update" project automates the extraction of email addresses and timestamps from server log files. Using Python and Regex, the extracted data is stored in MongoDB and later transferred to MySQL for structured analysis. The system helps track user activity trends, providing insights such as unique email addresses, email frequency per day, and domain-based email distribution.

## ğŸš€ Objectives  
- Extract email and timestamps from raw server logs.
- Store extracted data in MongoDB for flexible querying.
- Transfer data to MySQL for structured analysis.
- Provide insights into user activity trends.

## ğŸ› ï¸ Tools & Technologies  
- **Python**: ETL scripting, data processing  
- **Regex**: Extracting email addresses & timestamps  
- **MongoDB**: Storing unstructured extracted data  
- **MySQL / SQLite**: Structured database for querying  
- **pymongo, mysql-connector-python**: Database connectivity  

## ğŸ—ï¸ ETL Process  
1ï¸âƒ£ **Extract**: Read raw server logs, apply Regex to extract email and timestamps  
2ï¸âƒ£ **Transform**: Convert timestamps to a structured `datetime` format  
3ï¸âƒ£ **Load**: Store structured data into MongoDB and then MySQL for analysis  

## ğŸ“Š Data Insights  
- Number of unique email addresses  
- Emails count per day(Number of emails sent per day)  
- First and last email record activity per user  
- Domain-wise email distribution

**Conclusion & Future Enhancements**  
   âœ… Successfully automated log data extraction  
   âœ… Improved structured storage for efficient querying  
   ğŸš€ **Future Scope:** Add real-time log processing, integrate with cloud databases  

## ğŸ”§ How to Run  
```bash
# Clone the repository
git clone https://github.com/your_username/server-log-etl.git

# Install dependencies
pip install -r requirements.txt

# Run the ETL pipeline
python etl_pipeline.py
